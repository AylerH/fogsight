# 多模型支持功能

## 概述

Fogsight现在支持多种大语言模型，包括：

1. Google Gemini 2.5 Pro
2. 阿里云通义千问 2.5 (模拟模式)
3. 本地vllm部署的qwen模型
4. OpenAI兼容的API (如GPT-4, Claude等)

## 实现方式

系统会根据`credentials.json`中的`MODEL`配置自动选择使用哪种大模型：

- 如果`MODEL`包含"gemini"，则使用Google Gemini
- 如果`MODEL`包含"qwen"且设置了BASE_URL，则连接到本地vllm部署的qwen模型
- 如果`MODEL`包含"qwen"但没有设置BASE_URL，则使用阿里云通义千问模拟模式
- 如果API_KEY以"sk-"开头，则使用OpenAI兼容的API

## 配置示例

### Google Gemini

```json
{
    "API_KEY": "YOUR_GEMINI_API_KEY",
    "MODEL": "gemini-2.5-pro"
}
```

### 阿里云通义千问模拟模式

```json
{
    "API_KEY": "",
    "MODEL": "qwen2.5-72b-instruct"
}
```

通义千问模式下，系统会提供一个模拟响应，不会验证API密钥的有效性，也不会调用真实的API。这样可以确保系统在没有有效API密钥的情况下也能正常运行。

### 本地vllm部署的qwen模型

```json
{
    "API_KEY": "sk-no-key-required",
    "BASE_URL": "http://localhost:8000/v1",
    "MODEL": "qwen2.5-72b-instruct"
}
```

如果您有本地部署的vllm服务运行qwen模型，可以通过设置BASE_URL来连接到它。系统会使用OpenAI兼容的API格式与vllm服务通信。

### OpenAI兼容API

```json
{
    "API_KEY": "sk-YOUR_API_KEY",
    "BASE_URL": "https://api.openai.com/v1/",
    "MODEL": "gpt-4-turbo"
}
```

## 技术细节

1. **依赖管理**：
   - 添加了`google-generativeai`包支持Google Gemini
   - 添加了`dashscope`包支持阿里云通义千问
   - 使用`openai`包支持OpenAI兼容API和vllm

2. **代码结构**：
   - 在`app.py`中根据`MODEL`配置动态导入相应的包
   - 为每种模型实现了不同的流式生成逻辑
   - 通义千问模式下提供模拟响应，不验证API密钥
   - 系统只检查连通性，不验证API密钥的有效性
   - 支持连接到本地vllm部署的qwen模型

3. **错误处理**：
   - 提供通用错误信息，不暴露具体API密钥问题
   - 即使初始化LLM客户端失败，系统也会继续运行

4. **Docker支持**：
   - 更新了Dockerfile，确保安装所有必要的依赖
   - 添加了自动复制demo-credentials.json的逻辑

## 使用方法

1. 安装依赖：
   ```bash
   pip install -r requirements.txt
   ```

2. 配置credentials.json：
   ```bash
   cp demo-credentials.json credentials.json
   ```
   
3. 根据您要使用的模型编辑credentials.json

4. 启动应用：
   ```bash
   python app.py
   ```
   
   或使用Docker：
   ```bash
   docker-compose up -d
   ```

## 连接本地vllm服务

如果您想使用本地部署的vllm服务运行qwen模型，请按照以下步骤操作：

1. 确保您已经安装并启动了vllm服务，例如：
   ```bash
   python -m vllm.entrypoints.openai.api_server --model Qwen/Qwen2.5-72B-Instruct --host 0.0.0.0 --port 8000
   ```

2. 在credentials.json中设置以下配置：
   ```json
   {
       "API_KEY": "sk-no-key-required",
       "BASE_URL": "http://localhost:8000/v1",
       "MODEL": "qwen2.5-72b-instruct"
   }
   ```

3. 启动Fogsight应用，它将自动连接到您的vllm服务。 